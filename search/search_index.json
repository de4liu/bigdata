{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Resources for MSBA 6331 Big Data Analytics","text":"<p>This site hosts the resources for MSBA 6331 - Big Data Analytics at the Carlson School of Management, University of Minnesota, taught by Professor De Liu. </p> <p>Use the Menu or Search Box to navigate. </p>"},{"location":"datasets/","title":"Public Data Sets","text":"<p>This document is crowdsourced. Contributions are welcomed! Please feel free to suggest new data resources that you discover. </p>"},{"location":"datasets/#datagov","title":"Data.Gov","text":"<p>data.gov is the official US government open data site. Here you will find links to a wide variety of data sets in categories from agriculture to weather.</p>"},{"location":"datasets/#kaggle-datasets","title":"Kaggle Datasets","text":"<p>Kaggle Datasets </p>"},{"location":"datasets/#google-dataset-search","title":"Google Dataset Search","text":"<p>Google Dataset Search lets you find datasets wherever they are hosted, whether it\u2019s a publisher\u2019s site, a digital library, or an author\u2019s web page. It\u2019s a phenomenal dataset finder, and it contains over 25 million datasets.</p>"},{"location":"datasets/#uci-machine-learning-repository","title":"UCI Machine Learning Repository","text":"<p>UCI Machine Learning Repository: The Machine Learning Repository at UCI provides an up to date resource for open-source datasets.</p>"},{"location":"datasets/#cmu-libraries-dataset-for-machine-learning","title":"CMU Libraries Dataset for Machine Learning","text":"<p>CMU Libraries: Discover high-quality datasets thanks to the collection of Huajin Wang, at CMU. Also links to other data repositories.   </p>"},{"location":"datasets/#group-lens","title":"Group Lens","text":"<p>The University of Minnesota's GroupLens Project makes available large data sets on movies and ratings from the experimental MovieLens web site.  </p> <p>Many of the social networking sites publish streams of data for research purposes. Formats and available information vary widely. Some of them require you to register as a researcher, and potentially pay a fee, in order to gain access to the richest data.</p>"},{"location":"datasets/#student-contributed-data-sources-maintained-by-professor-reiley","title":"Student-Contributed Data Sources Maintained by Professor Reiley","text":"<p>This Google doc is an editable document with contributions from past students.</p>"},{"location":"datasets/#amazons-public-open-data-set","title":"Amazon\u2019s Public Open Data Set","text":"<p>This registry exists to help people discover and share datasets that are available via AWS resources. https://registry.opendata.aws/</p>"},{"location":"datasets/#google-big-query-public-datasets","title":"Google Big Query Public Datasets","text":"<p>https://cloud.google.com/bigquery/public-data/</p>"},{"location":"homework-guidelines/","title":"Homework Submission Guidelines","text":""},{"location":"homework-guidelines/#credit-sources-to-avoid-plagiarism","title":"Credit sources to avoid plagiarism","text":"<p>For short answers, if you use others' work as part of your answer, please properly cite your source. While it is fine to use web or other sources as a reference, you are prohibited from to lift whole paragraphs from the Internet. This is considered plagiarism, especially when you do not indicate the boundary of the copied content. When you quote a whole sentence or more, please add quotation marks around the copied content and indicate sources, so that we know you did not write that. </p> <p>Please refer to the following example for inline citation and bibliography styles. Please use the author-year format for inline citations. </p> <p>This phenomenon has been mentioned in several sources include a web page <sup>1</sup> and a journal paper (Yeh 1996). <sup>2</sup></p>"},{"location":"homework-guidelines/#generative-ai-policy","title":"Generative AI Policy","text":"<p>Generative AI (Gen AI) tools, such as ChatGPT, Copilot, DALL-E, Bard, and others, can be valuable resources for enhancing your learning experience. In this course, we also provide a SmartPal chatbot for course-specific Gen AI support. However, their use must align with the following guidelines to maintain academic integrity and ensure meaningful learning:</p> <p>You are encouraged to use Gen AI for non-exam assignments, including homework. You may use it to explore ideas, brainstorm solutions, or enhance your understanding.</p> <p>That said, you should use Gen AI responsibly:</p> <ul> <li>Accuracy and Relevance: AI outputs can be inaccurate or irrelevant. Always verify and refine the generated content to ensure it aligns with the course material and the specific question or context.</li> <li>Understanding: You must fully understand and be able to explain all submitted work. If asked during exams or discussions, you should demonstrate knowledge of the content without AI assistance.</li> <li>Original Perspectives: Incorporate your own ideas and analysis. Gen AI should support, not replace, your critical thinking and individual contribution.</li> </ul>"},{"location":"homework-guidelines/#submission-format","title":"Submission format","text":"<p>Submit all your answers in acceptable file formats (html, pdf, docx), preferably in one file. We do not accept ipynb because Canvas does not know how to render it.</p> <p>On Databricks, you can export your notebook as HTML and submit the HTML file.</p> <p>If your submission includes multiple files, attach them individually (do not zip it). </p>"},{"location":"homework-guidelines/#how-include-an-image-in-your-submission","title":"How include an image in your submission","text":"<p>If you need to upload a picture, we suggest you to upload the image to a free image hosting site https://freeimage.host/, and then copy the Markdown Full linked code into a markdown cell (a cell started with <code>%md</code>, as shown below, where the second line is coped from freeimage.host).</p> <p></p>"},{"location":"homework-guidelines/#how-to-take-a-screenshot","title":"How to take a screenshot","text":"<p>Occasionally you may need to submit a screenshot. Here are some tips</p> <p>Windows : In windows 7 or above, you may use the built-in Snipping tool , which allows you to select an area on screen and capture it. In Windows 10, you may use shortcut \"Win+Shift+S\" to select a screen area to save.</p> <p>Mac : On Mac OS, Press Command-Shift-4, and then drag the cross-hair pointer to select the area. </p> <ol> <li> <p>\"Zen and the Art of the Internet.\" http://freenet.buffalo.edu/~popmusic/zen10.txt\u00a0\u21a9</p> </li> <li> <p>Yeh, Michelle. \"The 'Cult of Poetry' in Contemporary China.\" Journal  of Asian Studies  55 (1996): 51-80.\u00a0\u21a9</p> </li> </ol>"},{"location":"kion/","title":"Kion Cloud Portal","text":"<p>Please follow these directions to login and start using your account: </p> <ol> <li>Navigate to kion.csom.umn.edu and click UMN Internet ID</li> <li>Provide your UMN email</li> <li>If you are asked to choose between Work and Personal accounts, choose Work or School Account. </li> <li>You'll see a KION dashboard</li> <li>Find the project CSOM-MSBA-6331 (Spring 2025), click the cloud icon </li> <li>Click Cloud Access Roles and select the StudentAdministrator role </li> <li>Choose web access to access the AWS console </li> <li>Switch to United States (N. Virginia) (us-east-1) if the current region is not. </li> <li>Once there, you can access Bedrock via the console. <ul> <li>Claude 3.7/3.5 models have been enabled by default.</li> <li>If you need other models, go to  Bedrock configurations &gt; Model access &gt; Modify model access &gt; click Available to request of the model request a model to added. </li> </ul> </li> <li>Obtain Access Token (for API access) - if you need it. <ul> <li>At the KION project CSOM-MSBA-6331 (Spring 2025), select access type Short-term Access Keys to view access keys and instructions. Click on Generate to generate new ones.   </li> </ul> </li> </ol>"},{"location":"lab_appendix/","title":"Lab Dataset Reference: Dualcore","text":""},{"location":"lab_appendix/#tables-imported-from-mysql","title":"Tables Imported from MySQL","text":"<p>The following depicts the structure of the MySQL tables imported into HDFS using Sqoop. The primary key column from the database, if any, is denoted by bold text:</p>"},{"location":"lab_appendix/#customers","title":"customers","text":"<p>201,375 records (imported to <code>/dualcore/customers</code>) </p> Index Field Description Example 0 cust_id Customer ID <code>1846532</code> 1 fname First name <code>Sam</code> 2 lname Last name <code>Jones</code> 3 address Address of residence <code>456 Clue Road</code> 4 city City Silicon <code>Sands</code> 5 state State <code>CA</code> 6 zipcode Postal code <code>94306</code>"},{"location":"lab_appendix/#employees","title":"employees","text":"<p>61,712  records (imported to <code>/dualcore/employees</code>and later used as an external table in Hive) </p> Index Field Description Example 0 emp_id Employee ID <code>BR</code> 1 fname First name <code>Betty</code> 2 lname Last name <code>Richardson</code> 3 address Address of residence <code>123 Shady Lane</code> 4 city City <code>Anytown</code> 5 state State <code>CA</code> 6 zipcode Postal Code <code>90210</code> 7 job_title Employee\u2019s job title <code>Vice President</code> 8 email eEmail address <code>br5331404@example.com</code> 9 active Is actively employed? <code>Y</code> 10 salary Annual pay (in dollars) <code>136900</code>"},{"location":"lab_appendix/#orders","title":"orders","text":"<p>1,662,951  records (imported to <code>/dualcore/orders</code>) </p> Index Field Description Example 0 order_id Order ID <code>3213254</code> 1 cust_id Customer ID <code>1846532</code> 2 order_date Date/time of order 2013-05-31 16:59:<code>34</code>"},{"location":"lab_appendix/#order_details","title":"order_details","text":"<p>3,333,244 records (imported to <code>/dualcore/order_details</code>) </p> Index Field Description Example 0 order_id Order ID <code>3213254</code> 1 prod_id Product ID <code>1754836</code>"},{"location":"lab_appendix/#products","title":"products","text":"<p>1,114 records (imported to <code>/dualcore/products</code>) </p> Index Field Description Example 0 prod_id Product ID <code>1273641</code> 1 brand Brand name <code>Foocorp</code> 2 name Name of product 4 - port <code>USB Hub</code> 3 price Retail sales price, in cents <code>1999</code> 4 cost Wholesale cost, in cents <code>1463</code> 5 shipping_wt Shipping weight (in pounds) <code>1</code>"},{"location":"lab_appendix/#suppliers","title":"suppliers","text":"<p>66  records (imported to <code>/dualcore/suppliers</code>) </p> Index Field Description Example 0 supp_id Supplier ID <code>1000</code> 1 fname First name <code>ACME Inc.</code> 2 lname Last name <code>Sally Jones</code> 3 address Address of office <code>123 Oak Street</code> 4 city City <code>New Athens</code> 5 state State <code>IL</code> 6 zipcode Postal code <code>62264</code> 7 phone Office phone number <code>(618)555-5914</code>"},{"location":"lab_appendix/#hive-tables","title":"Hive Tables","text":"<p>The following is a record count for Hive tables that are created or queried during the hands-on exercises. Use the <code>DESCRIBE tablename</code>  command in Hive to see the table structure. </p> Table Name Record  Count cart_items <code>33,812</code> cart_orders <code>12,955</code> cart_shipping <code>12,955</code> cart_zipcodes <code>12,955</code> checkout_sessions <code>12,955</code> customers <code>201,375</code> employees <code>61,712</code> order_details <code>3,333,244</code> orders <code>1,662,951</code> products <code>1,114</code> ratings <code>21,997</code> web_logs <code>412,860</code>"},{"location":"lab_appendix/#other-data-added-to-hdfs","title":"Other Data Added to HDFS","text":"<p>The following describes the structure of other important data sets added to HDFS. </p>"},{"location":"lab_appendix/#combined-ad-campaign-data","title":"Combined Ad Campaign Data","text":"<p>(788,952 records total)</p> <p>stored in two directories: </p> <ul> <li><code>/dualcore/ad_data1</code>( 438,389  records)  </li> <li><code>/dualcore/ad_data2</code>( 350,563  records). </li> </ul> Index Field Description Example 0 campaign_id Uniquely identifies our ad <code>A3</code> 1 date Date of ad display <code>05/23/2013</code> 2 time Time of ad display <code>15:39:26</code> 3 keyword Keyword that triggered ad <code>tablet</code> 4 display_site Domain where ad shown <code>news.example.com</code> 5 placement Location of ad on Web page <code>INLINE</code> 6 was_clicked Whether ad was clicked <code>1</code> 7 cpc Cost  per click, in cents <code>106</code>"},{"location":"lab_appendix/#accesslog","title":"access.log","text":"<p>412,860  records (uploaded to <code>/dualcore/access.log</code>) </p> <p>This file is used to populate the <code>web_logs</code>table in Hive. Note that the RFC 931 and Username fields are seldom populated in log files for modern public Web sites and are ignored in our RegexSerDe. </p> Index Field Description Example 0 IP address <code>192.168.1.15</code> 1 RFC 931 (Ident) <code>-</code> 2 Username <code>-</code> 3 Date/Time <code>[22/May/2013:15:01:46 -0800]</code> 4 Request <code>\"GET /foo?bar=1 HTTP/1.1\"</code> 5 Status code <code>200</code> 6 Bytes transferred <code>762</code> 7 Referer <code>\"http://dualcore.com/\"</code> 8 User agent (browser) <code>\"Mozilla/4.0 [en] (WinNT; I)\"</code> 9 Cookie (session ID) <code>\"SESSION=8763723145\"</code>"},{"location":"prework/","title":"Prework","text":""},{"location":"prework/#sign-up-for-databricks-community-edition","title":"Sign up for Databricks Community Edition","text":"<p>Databricks is the company behind Apache Spark. The community edition offers you a cost-free environment (15GB RAM and 2 Cores) with the latest Apache Spark &amp; much more. </p> <ul> <li>To sign up for Databricks Community Edition, follow the instructions here: https://docs.databricks.com/en/getting-started/community-edition.html<ul> <li>We recommend you use UMN email for the sign up. </li> <li>On the page, \"How do you plan on using Databricks?\", choose the Community Edition.</li> <li>At the final page, be sure to choose the \"Get started with Community Edition\" (the grey button instead of the red button) -- see a picture below. </li> <li>As suggested, after signing in, you may try the Get started: Query and visualize data from a notebook to familiarize yourself with Databricks.</li> </ul> </li> </ul> <p></p> <ul> <li>After that, you can bookmark this page community.cloud.databricks.com for future log on.</li> </ul>"},{"location":"prework/#linux-shell-commands","title":"Linux Shell Commands","text":"<p>We will spend some of our course time on Linux shell commands, which are often used in big data, cloud computing, and MLOps. It would be helpful for you to warm up to the concepts and commands with the following video and short web course. </p> <p>Here are some materials that warm you up to the concepts:</p> <ul> <li>Learning the Shell (web tutorial): just focus on <code>Learning the Shell</code> chapter</li> <li>Introduction to Linux Operating System (A gentle overview): only need to watch the segment of 18:05-52:03 (or the following topics)<ul> <li>18:05 Introduction to Linux operating system and comparison with windows</li> <li>24:32 Terminal vs. File Manager</li> <li>27:20 Command Line Interfaces on Ubuntu Operating system</li> <li>49:19 Brief of Linux commands</li> </ul> </li> </ul>"},{"location":"prework/#some-useful-concepts-in-computing","title":"Some Useful Concepts in Computing","text":"<p>Big data is about completing tasks in parallel, using multiple computers. Naturally, it needs some background on how computing is done on a single computer.  These are a series of short videos explaining various concepts and aspects of computing.</p> <ul> <li>https://www.youtube.com/watch?v=p3q5zWCw8J4 (how do computer memory work)</li> <li>https://www.youtube.com/watch?v=H_M--weEzpA (Memory versus Storage)</li> <li>https://www.youtube.com/watch?v=IIvbEn54ZAY (CPUs and Cores)</li> <li>https://www.youtube.com/watch?v=H4l42nbYmrU (video explaining ASCII files)</li> <li>https://www.youtube.com/watch?v=v7IpCq5YL68 (comparing plain text and binary files)</li> <li>https://www.youtube.com/watch?v=BKgRaHMUul0 (explaining binary files and demonstrating difference in file types)</li> </ul>"},{"location":"trendsMarket/","title":"Big Data and AI Trends Market, Spring 2025","text":"<p>Draft revision: 2025-02-23</p> <p>Time/Location: April 25, 3:00-4:30pm Humphrey</p>"},{"location":"trendsMarket/#1.-project-goals-and-requirements","title":"Goals and Requirements","text":"<p>The fields of big data, cloud computing, and AI are fast-moving and expertise in those areas are in demand. Many companies leverage scalable infrastructure to handle structured data, semi-structured, and unstructured data, and address data volume, variety, velocity, and veracity challenges using modern tools, pipelines, and platforms (especially cloud-based ones). This year's theme will encompass AI (especially Generative AI) - teams are encouraged to explore Gen AI &amp; AI applications. </p> <p>The trend marketplace project aims for two types of goals. Your team may place emphasis on either one or both of them: </p> <ul> <li> <p>Goal A. (Problem Solving with Big Data/AI) Identify a meaningful business problem and real-world dataset that could be leveraged to address the problem. You team will implement an appealing big data/AI solution to solve the problem. </p> </li> <li> <p>Goal B. (Explore Novel Big Data/AI Technologies) Learn and research a novel big data/AI technology. Through research about the technology and building of a prototypical use case(s) using the technology, your gain understanding of the technology and its applications and its merits/shortcomings.</p> </li> </ul>"},{"location":"trendsMarket/#1.1-business-value-requirement","title":"Business Value","text":"<p>Your project should speak to the business needs of an intended audience. The intended audience could be a specific business or a population (e.g. business analytics students, a marketing team). I expect that in your hand out that you will highlight your big data/AI technology/solution adds value for your intended audience.</p>"},{"location":"trendsMarket/#1.2-data-requirement","title":"Data","text":"<p>The most effective way of practicing data skills is to work with real world data. The project will benefit you the most if you choose to work with a dataset that ideally has one or a few of the following big data characteristics that allow you to practice big data/AI skills. </p> <ul> <li>Volume: for project purpose, a large data set means a data set that has a few millions records and/or a few gigabytes (this is less than what real-world big data is, but we may not have the resources to handle even bigger datasets), or  </li> <li>Velocity: you are dealing with streaming data, or  </li> <li>Variety: you are dealing with semi-structured or unstructured data such as Json, logs, text, images, documents, audio, videos, </li> <li>Veracity: your are dealing with real-world data that has biases, noise, and abnormality. As a result, you are implementing a data pipeline to perform essential data cleaning, inspection, transformation, and augmentation steps before the data is good enough for use. </li> </ul>"},{"location":"trendsMarket/#1.3-technology-requirement","title":"Technology","text":"<p>Every team project is expected to leverage big data/AI technologies, be it technologies we cover in the course (e.g. Hive, Spark, Cloud technologies) or ones we did not cover. Technologies are interpreted broadly, which may include (but not limited to) big data, cloud computing, Gen AI, and NoSQL technologies. Projects completed following MLOps practices are valued.</p> <p>Ideally, your entire data pipeline is built to scale, but I also accept projects that have some (but not all) steps that use non-scalable technologies. </p> <p>Carlson IT will collaborate with us to potentially provide access to some cloud computing resources on a case-by-case basis. You have give them enough lead time to make it work (more details to come).</p>"},{"location":"trendsMarket/#teams","title":"Teams","text":"<p>Instructor will form randomized teams of 5-6, and may balance different backgrounds in doing so. </p>"},{"location":"trendsMarket/#2.-milestones","title":"Deliverables","text":"<p>This project has two milestones: proposal and final deliverables. See course schedule for due dates of these milestones. </p>"},{"location":"trendsMarket/#2.1-proposal","title":"Project Proposal","text":"<p>The first milestone is to submit your project proposal to get instructor feedback. </p> <ul> <li>Proposal is not graded, and is only for obtaining feedback and approval. </li> <li>Please submit your project proposal draft l through the instructor designated google doc (a separate doc for each team).  </li> <li>The approved proposal/abstract will be shared in this Google doc.</li> </ul> <p>Your proposal should have these elements</p> <ul> <li>Team number</li> <li>Members</li> <li>Project Title</li> <li> <p>The project description depends on your project goal:</p> <ul> <li> <p>Goal A - Problem Solving with Big Data/AI: </p> <ul> <li>What is the topic of your project </li> <li>what kinds of data you will use (how you get it, provide links to data sources). </li> <li>What kinds of analysis do you plan to do? Who is your target audience? </li> <li>which tools will be leveraged for major steps of data engineering and analysis (ingestion, ETL, exploration, model building, deployment etc).     </li> </ul> </li> <li> <p>Goal B - Explore Novel Big Data/AI Technologies </p> <ul> <li>what kind of technology you plan to focus on, </li> <li>what you plan to cover in your demonstration, </li> <li>where resources you plan to draw upon (books, videos, PPTs, articles etc - so that the instructor can also evaluate the feasibility and interestingness). </li> <li>If you proposal includes a use case, describe what kinds of analysis you plan to do, including links to possible data sources.</li> </ul> </li> </ul> </li> </ul> <p>The project proposal workflow is as follows:</p> <ol> <li>Enter a draft proposal in the designated Google doc for your team</li> <li>Contact the instructors for feedback (with your google doc as an attachment).<ul> <li>Initially, you may present 2+ options to get an instructor opinion on which one is preferred. </li> <li>You may also verbally talk to the instructors and/or schedule an appointment.   </li> </ul> </li> <li>The instructor will give feedback, including how/whether you should proceed. </li> <li>If a revision is required, you should revise your proposal and repeat steps 1-3. </li> </ol> <p>Once the project starts, if your team requires assistance from the instructor, please make an appointment on a case-by-case basis. </p>"},{"location":"trendsMarket/#2.2-trend-marketplace","title":"Event Day Deliverables","text":"<p>On the day of the Trends Market, each team will have a table to showcase their work for peers, external evaluators, and professors, and prepare to give a short presentation with Q\\&amp;A. </p> <p>Prior to the day of the Trend Marketplace, the team will need to prepare the following material:</p> <ul> <li>A two-page / 1 sheet handout that summarizes the project, data, methodology, results/takeaways.<ul> <li>The handout should clearly mark team Number and member names.   </li> </ul> </li> <li>Prepare a short (&lt; 5 minute) briefing that includes:<ul> <li>A demonstration of the technology or solution, or</li> <li>A slideshow or poster board summary</li> </ul> </li> </ul>"},{"location":"trendsMarket/#2.4-final-deliverable","title":"Post-event Final deliverable","text":"<ul> <li>Please submit your github link to the  Google doc as well as the Trends Market assignment on canvas before the submission deadline.</li> </ul> <p>Github.com Repository</p> <p>A git repository is a preferred way of showcasing your skill sets (e.g. to potential employers). The git repository will host your project materials. It is also a place for a more technical audience to dig in and learn from your work.</p> <p>Your git repository should include the following components:</p> <ul> <li>(required) A README.md markdown file that serves as a project homepage and introduction/executive summary. It also provides links to more project-related materials, e.g. flier, links to dataset, links to relevant articles/resources. The readme should mention that \u201cThis project repository is created in partial fulfillment of the requirements for the Big Data Analytics course offered by the Master of Science in Business Analytics program at the Carlson School of Management, University of Minnesota.\u201d   </li> <li>(required) An instruction: provide instructions on how to use (reuse) this project\u2019s codes (e.g. setup/installation, commands, steps, requirements etc).  </li> <li>(required) Project scripts (commands, python scripts, jupyter notebooks, SQL queries etc) that you use in the project with proper documentation/comments for ease of understanding. Please note that the code should be free of password and credentials (you should not commit such confidential information to the repository to begin with). </li> <li>(required) The flier (pdf).  </li> <li>(optional) bibliography and credits: give credits to sources that you use (e.g.,  papers/articles, web pages, data source, git repository etc). This is part of the scholarly honesty requirements and allows the instructor to evaluate the amount/quality of work the team has done.   </li> <li>(optional) additional resources (e.g., pdf documents and/or links to external resources).  </li> <li>(optional) sample data (small data, but not the whole data because github is not meant for storing large quantities of data). Note: big files should be submitted via Google drive. </li> </ul> <p>Here is a simple guide on how to organize your data science git repo</p>"},{"location":"trendsMarket/#3.-evaluation","title":"Grading","text":"<p>We use the following evaluation methods:</p> <ul> <li>(10%) Completing your deliverables (project flier, git repository, evaluation of other teams &amp; your teammates)  </li> <li>(40%) Peer evaluation. Each student will evaluate at least two other team\u2019s projects based on the team\u2019s project flier, git repository, and booth visit.  </li> <li>(25%) Instructor evaluation. The instructor will evaluate each project based on the team\u2019s team\u2019s project flier, and git repository.  </li> <li>(25%) External evaluation. External evaluators\u2019 evaluation of the project.   </li> <li>(5% bonus) Best in show: Each team will submit a consensus list of their picks for the top 3 projects of the entire session.</li> </ul> <p>Evaluations will generally use the following criteria:</p> <ul> <li>Business value (20%): The project topic is meaningful/interesting for intended audience. The problem being addressed is relevant, with clear potential impact.</li> <li>Technical Quality (20%): The technical approach of the project is sound and appropriate. </li> <li>Presentation (30%): The presentation is clear, effective, and well organized. Instructor evaluation may take into account the organization and completeness of the team's github repository. </li> <li>Novelty (20%): The team introduces a technology, solution approach, or problem domain. The project demonstrates creativity and innovation. </li> <li>Professionalism (10%): The team conducts themselves professionally in presentations and interactions. </li> </ul> <p>Teammate Evaluation </p> <ul> <li>Member contribution: We will collect your anonymous evaluation of your teammates. While in general every team member receives the same project credits, those who lack contribution (as reflected by teammate evaluation and comments) could receive partial to zero credit for the team project.   </li> <li>Professionalism: The team members will also submit anonymous evaluations of each other in terms of professionalism in the collaboration process. The instructor reserves the right to take measures to address professionalism issues including imposing a penalty on members who violate the code of professionalism.</li> </ul>"},{"location":"trendsMarket/#1.4-sample-projects-from-the-past","title":"Sample Past Projects","text":"<p>See below for sample projects from previous years\u2019 full-time MSBA students (please note that there are a number of stream projects because we had time to get a bit into streaming during the fall semester). You are welcome to reuse and expand on these projects (but you should always give credits to prior work including what you\u2019ve found on the Internet). </p> <p>2023 Projects </p> <p>2022 Projects </p> <p>2021 Projects</p> <p>2019 Projects</p>"},{"location":"trendsMarket/#public-datasets","title":"Public Datasets","text":"<p>Check out a curated list of public datasets here.</p>"},{"location":"FAQs/addimage/","title":"Embed Images to Jupyter Notebook","text":""},{"location":"FAQs/addimage/#simplest-workflow","title":"Simplest workflow","text":"<ol> <li>upload the image (most commonly jpg or png) to your directory</li> <li> <p>In a markdown cell, add <code>![alternative text](path-to-image-file)</code> to display the image.</p> </li> <li> <p>Export the Jupyter Notebook to PDF and submit the PDF</p> </li> </ol>"},{"location":"FAQs/addimage/#in-the-event-that-pdf-converter-encounters-an-error-you-may-either","title":"In the event that PDF converter encounters an error, you may either","text":""},{"location":"FAQs/addimage/#embed-image-in-html","title":"Embed image in html","text":"<ol> <li> <p>convert your image using base64 encoder using online services https://www.base64-image.de/ </p> </li> <li> <p>In a markdown cell, enter an image tag <code>&lt;img src=\"data:image/png;base64\"/&gt;</code>, and replace the <code>data:image/...</code> portion with your encoded image from your encoder. The image should display in the rendered markdown cell. </p> </li> <li> <p>Export the HTML and submit it.</p> </li> </ol> <p>The reason that we have to embed the image is because HTML files link to images rather than embed them by default. Even though you have uploaded your image to your workspace, I cannot access this file in your space. So when you convert it to HTML and view it in a place other than your server, the image cannot be displayed. Embedding forces the HTML to store the image data within the HTML file.</p>"},{"location":"FAQs/addimage/#submit-html-and-images-separately","title":"Submit HTML and image(s) separately.","text":"<ol> <li>Alternatively, you can submit your HTML as well as your images to Canvas.</li> </ol>"},{"location":"FAQs/cheatsheet_git/","title":"Git Cheatsheet","text":"<p>Note: you may need to prefix git commands with <code>sudo</code> when you use git in the VM environment. <code>sudo</code> give you the necessary privilege for certain git operations.</p> <p>clone the repository</p> <pre><code># accept default folder name [vagrant]\ngit clone https://github.umn.edu/deliu/vagrant\n\n# use your own folder name\ngit clone https://github.umn.edu/deliu/vagrant myfoldername\n\n# On VM, you need to provide your userid and add sudo, you'll be asked your U of M password\nsudo git clone https://myuserid@github.umn.edu/deliu/vagrant\n</code></pre> <p>Add files to the repository</p> <pre><code>git add file1.txt\ngit add file2.gif\ngit add *.py\n</code></pre> <p>Add all (new and changed) files to the repository</p> <pre><code>git add -A\n</code></pre> <p>Review your repository status</p> <pre><code>git status\n</code></pre> <p>Commit the changes (the description is required)</p> <pre><code>git commit -m \"Description of my change\"\n</code></pre> <p>Push them back to the remote repository</p> <pre><code>git push\n</code></pre> <p>Get latest version - you should do this before you make your changes</p> <pre><code>git pull\n</code></pre> <p>To undo changes you have done to the files (that you have not committed). </p> <pre><code>git checkout\n</code></pre>"},{"location":"FAQs/configGit/","title":"Config Git","text":"<p>After you install Git, it takes a few simple steps to begin using it:</p> <p>At the command line:</p> <pre><code># configure your name\ngit config --global user.name \"myname\"\n# email\ngit config --global user.email \"myemail@umn.edu\"\n# when you type your password, it is cache for long time\ngit config --global credential.helper \"cache --timeout=99999\"\n</code></pre>"},{"location":"FAQs/debug_hadoop/","title":"Debug MapReduce Jobs","text":"<p>When you work with Hadoop MapReduce jobs for this course, here are some of the tips for debugging your Hadoop MapReduce issues:</p> <ol> <li>Do your input files exist, and in the format expected?<ul> <li>You can use <code>hadoop fs -cat file | head</code> command to find this out.</li> </ul> </li> <li>Does your output folder already exist?<ul> <li>Hadoop won't run if  the output folder already exists.</li> <li>Use <code>hadoop fs -rm -r -f output_dir</code> to remove the output folder.</li> </ul> </li> <li>Does your python mapper/reducer have syntax or run-time errors?<ul> <li>You can debug your mapper and reducer programs using Linux pipes. </li> </ul> </li> <li>If you pass step 3, does your mapper/reducer file has wrong line endings?<ul> <li>Follow tips here to view and convert line endings.</li> </ul> </li> </ol>"},{"location":"FAQs/debug_hadoop_streaming/","title":"Debug Hadoop Streaming Jobs","text":"<p>When you work with Hadoop MapReduce streaming jobs, it may be a good idea to debug your mapper and reducer codes using Linux pipes such as below. </p> <pre><code>hadoop fs -cat input_data | head -n 100 | python mapper.py | sort | python reducer.py\n</code></pre> <p>This is how Hadoop Streaming will use these programs. This will let you quickly see bugs in your python program without the overhead of invoking MapReduce. </p> <p>Hope this helps!</p>"},{"location":"FAQs/edit_linux_file/","title":"Edit files in Linux","text":"<p>This tutorial explains how you can edit files in Linux. </p>"},{"location":"FAQs/edit_linux_file/#approach-1-using-linux-command-line-tools","title":"Approach 1. Using Linux command line tools.","text":"<p>In our environment, you can use command-line based text editors including <code>nano</code> and <code>vi</code> to edit something small. <code>vi</code> is briefly explained. <code>nano</code> is relative intuitive to learn and use.</p> <p>You can type:</p> <p><code>nano file.txt</code></p> <p>to open or create a file. Edit it, and then follow the shortcut at the bottom of your screen for file operations, including</p> <ul> <li>Ctrl+O: write out (i.e. Save)</li> <li>Ctrl+X: exit Nano </li> </ul> <p>If you are using Gitbash for this, you can use your mouse to select a block of text (which is automatically copied), and right click to paste.</p>"},{"location":"FAQs/edit_linux_file/#approach-2-using-linuxs-gedit-tool","title":"Approach 2. Using Linux's Gedit tool","text":"<p>Open Virtualbox, click play to open a GUI on your running VM.</p> <p>From the menu, go to Applications &gt; Accessories &gt; Gedit text editor</p> <p>Then you can edit and save. Keep in mind that your user name and home directory is \"cloudera\". </p> <p>Gedit is versatile; it can be used edit txt files and scripts (such as python)</p>"},{"location":"FAQs/edit_linux_file/#approach-3-using-windows-tools","title":"Approach 3. Using Windows tools","text":"<p>You can take advantage of the fact that we have a shared folder between window host and the VM. This shared folder is at <code>c:/vagrant</code> on the windows side, and <code>/vagrant</code> on the Linux side. the files you put in this folder can be seen from both sides.</p> <p>Use notepad++ to edit a text file or script file and save to <code>c:/vagrant</code>.</p> <p>To ensure compatibility, it is best for you to save the text file with Linux line endings (by default it is windows line endings). To do that, go to Edit &gt; EOL conversion &gt; Unix (LF) to set it to use Linux line endings. See more about line ending conversion here.</p> <p>Once you have the file in the vagrant folder, you can easily copy or move it to a new location in Linux. e.g.</p> <p><code>cp myscript.py ~/training_materials/analyst/myscript.py</code></p> <p>The same approach can also be used for moving data files between the two systems.</p>"},{"location":"FAQs/hive_debug/","title":"Fix Hive metastore db error","text":"<p>If you encounter this error:</p> <p><code>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</code></p> <p>It could be either:</p> <ul> <li>You have other Hive instances (e.g. a Hive shell) running that locks the access to Hive metastore.</li> <li>Hive metastore is corrupt or not initialized properly.</li> </ul> <p>Please note that because Hive's metastore database is powered by a single-user embedded database called Derby, you cannot run two Hive instances at once. Be sure to close other instances of Hive (or Hive Shell) before you run Hive.</p> <p>Please take the follow steps:</p> <ol> <li>Close all other Hive instances (hive shells) and try again. This includes closing all notebooks and terminals but the current one. The second icon (a circle with a square hole) on the left panel will show all the running notebooks and terminals with a \"Shut down\" button.</li> <li>If the error persists, you can try to reinitialize Hive by running this hive setup script <code>~/MSBA6330/uploads/setup_hive.sh</code>. Note that, after this, you also need to reinstall the databases that you need (see relevant lab instructions)</li> <li>You may restart the course server (by going to menu: file &gt; hub control panel &gt; stop server )</li> </ol>"},{"location":"FAQs/homework-guidelines/","title":"Homework Submission Guidelines","text":""},{"location":"FAQs/homework-guidelines/#1-credit-sources-to-avoid-plagiarism","title":"1. Credit sources to avoid plagiarism","text":"<p>For short answers, if you use others' work as part of your answer, please properly cite your source. While it is fine to use web or other sources as a reference, you are prohibited from to lift whole paragraphs from the Internet. This is considered plagiarism, especially when you do not indicate the boundary of the copied content. When you quote a whole sentence or more, please add quotation marks around the copied content and indicate sources, so that we know you did not write that. </p> <p>Please refer to the following example for inline citation and bibliography styles. Please use the author-year format for inline citations. </p> <p>This phenomenon has been mentioned in several sources include a web page <sup>1</sup> and a journal paper (Yeh 1996). <sup>2</sup></p>"},{"location":"FAQs/homework-guidelines/#2-generative-ai-policy","title":"2. Generative AI Policy","text":"<p>Generative AI (Gen AI) tools, such as ChatGPT, Copilot, DALL-E, Bard, and others, can be valuable resources for enhancing your learning experience. In this course, we also provide a SmartPal chatbot for course-specific Gen AI support. However, their use must align with the following guidelines to maintain academic integrity and ensure meaningful learning:</p> <p>You are encouraged to use Gen AI for non-exam assignments, including homework. You may use it to explore ideas, brainstorm solutions, or enhance your understanding.</p> <p>That said, you should use Gen AI responsibly:</p> <ul> <li>Accuracy and Relevance: AI outputs can be inaccurate or irrelevant. Always verify and refine the generated content to ensure it aligns with the course material and the specific question or context.</li> <li>Understanding: You must fully understand and be able to explain all submitted work. If asked during exams or discussions, you should demonstrate knowledge of the content without AI assistance.</li> <li>Original Perspectives: Incorporate your own ideas and analysis. Gen AI should support, not replace, your critical thinking and individual contribution.</li> </ul>"},{"location":"FAQs/homework-guidelines/#3-submission-format","title":"3. Submission format","text":"<p>Submit all your answers in acceptable file formats (html, pdf, docx), preferably in one file. We do not accept ipynb because Canvas does not know how to render it.</p> <p>On Databricks, you can export your notebook as HTML and submit the HTML file.</p> <p>If your submission includes multiple files, attach them individually (do not zip it). </p>"},{"location":"FAQs/homework-guidelines/#4-how-include-an-image-in-your-submission","title":"4. How include an image in your submission","text":"<p>If you need to upload a picture, we suggest you to upload the image to a free image hosting site https://freeimage.host/, and then copy the Markdown Full linked code into a markdown cell (a cell started with <code>%md</code>, as shown below, where the second line is coped from freeimage.host).</p> <p></p>"},{"location":"FAQs/homework-guidelines/#5-how-to-take-a-screenshot","title":"5. How to take a screenshot","text":"<p>Occasionally you may need to submit a screenshot. Here are some tips</p> <p>Windows : In windows 7 or above, you may use the built-in Snipping tool , which allows you to select an area on screen and capture it. In Windows 10, you may use shortcut \"Win+Shift+S\" to select a screen area to save.</p> <p>Mac : On Mac OS, Press Command-Shift-4, and then drag the cross-hair pointer to select the area. </p> <ol> <li> <p>\"Zen and the Art of the Internet.\" http://freenet.buffalo.edu/~popmusic/zen10.txt\u00a0\u21a9</p> </li> <li> <p>Yeh, Michelle. \"The 'Cult of Poetry' in Contemporary China.\" Journal  of Asian Studies  55 (1996): 51-80.\u00a0\u21a9</p> </li> </ol>"},{"location":"FAQs/installLocalSpark/","title":"Install Spark Locally on your Own Computer","text":"<p>We mainly used a virtual machine for Machine Learning/Data Science tasks on Github to setup a local Spark environment.</p> <p>Below are the steps</p> <ol> <li>First visit the git repository for the Spark Virtual Machine (VM) and take a look what this virtual machine offers.</li> <li>Set up the Spark directory on your computer by either using git clone (if you have git installed on your machine), or by downloading a copy of the above git repository. To avoid trouble, we advise you use a directory name without spaces. I recommend use <code>c:/spark</code> for windows machines.</li> <li>Install the latest version of vagrant at  https://www.vagrantup.com/downloads.</li> <li>Install the VirtualBox 6.0 version at https://www.virtualbox.org/wiki/Download_Old_Builds_6_0. Please note that the newest version of VirtualBox may not be supported by vagrant yet. </li> <li>Provision the virtual machine from a command window by CD the directory you have the Spark vm files first (e.g. <code>c:/spark</code>), then execute the following command: <pre><code># brings up the VM\nvagrant up\n</code></pre></li> </ol> <p>This will initialize and start the VM, which will take several minutes (subsequent booting time will be shorter). It downloads a base VM from the Internet and then augments it with the script specified in  the <code>Vagrantfile</code> in the Spark folder.</p> <ol> <li>If the provision succeeded, you can bring up a PySpark notebook by entering localhost:8008 in a browser window. Enter \"vmuser\" for both user name and password. </li> </ol> <p>You may upload a new notebook using the upload button from the jupyter notebook. The uploaded notebook will be located at  <code>c:\\spark\\vmfiles\\IPNB\\</code> (assuming your spark folder is <code>c:/spark</code>) on your computer. </p> <p>Please note that when you open the notebook, you will most likely need to manually specify the kernel to be \"PySpark (py3)\" to use Spark functionalities. </p> <p></p>"},{"location":"FAQs/installLocalSpark/#faqs","title":"FAQs","text":""},{"location":"FAQs/installLocalSpark/#1-virtualization-vt-x-is-disabled-on-your-host-machine","title":"1. Virtualization (VT-x) is disabled on your host machine","text":"<p>During the VM installation, you may fail to bring up the VM with an error message like one of the following:</p> <p>VT-x is disabled in the BIOS for all CPU modes. Binary translation is incompatible with long mode on this platform. Long mode will be disabled in this virtual environment. This virtual machine is configured for 64-bit guest operating systems. However, 64-bit operation is not possible.</p> <p>The likely cause is that Virtualization (VT-x) is not enable on your machine. VirtualBox will not play the VM if the virtulization (<code>VT-x</code>) is disabled on your host machine. </p> <p>To enable virtualization, you need to access the BIOS of your machine. Depending on the machine model, it may requires you to long press a function key such as F8, F12, and F2 during boot up. You should search for \"how to access BIOS [YOUR COMPUTER MODEL]\" for instructions specific to your computer model. After getting into to BIOS setup, you should look for an configuration that mentions <code>Intel Virtual Technology</code>, <code>VT-x</code>, <code>Virtualization</code> from the menu, then enable it. </p>"},{"location":"FAQs/line_endings/","title":"Issues with Line Endings in Text Files","text":"<p>Windows text files by default have line endings of <code>\\r\\n</code>, but linux/unix text files have line endings of <code>\\n</code>. Often times, files originated from windows may not work for linux because of this (e.g. for python-based mappers and reducers). </p>"},{"location":"FAQs/line_endings/#how-would-i-know-the-line-endings-were-rn-instead-of-n","title":"How would I know the line endings were <code>\\r\\n</code> instead of <code>\\n</code>?","text":"<p>A quick way to see if the file has windows line ending is through vim. <pre><code>vi file.py \n</code></pre> type <code>:set list</code> to see line endings.  If it is windows line endings, you will see <code>^M</code> at end of each line. Type <code>:q</code> to exit the vi editor.</p>"},{"location":"FAQs/line_endings/#how-to-fix-it","title":"How to fix it?","text":"<p>To replace windows line endings with linux ones, use linux command: <pre><code>sed -i 's/\\r//g' file.py\n</code></pre></p>"},{"location":"FAQs/line_endings/#how-to-prevent-this","title":"How to prevent this?","text":"<p>If you have to edit a file in windows, please create the file in Linux or copy an existing linux-originated file (so it has the right endings), then edit it.</p> <ul> <li>Notepad++: You can view line endings using tool bar button \"\u00b6\". At the status bar, you can switch between \"Unix(LF)\" and \"Windows (CR LF)\".</li> <li>Sublime Text: This page explains how to set default line endings to linux. Here is how you may convert it.</li> </ul>"},{"location":"FAQs/markdown2pdf/","title":"Convert Markdown to PDF","text":"<p>Our assignments are distributed in markdown files. If you want to leverage the markdown file, you will need a workflow that converts markdown to pdf. Here are some of the options:</p> <ul> <li>Convert Markdown to PDF</li> <li>1. Online Markdown to PDF Converter</li> <li>2. R-Studio - Requires Pandoc</li> <li>3. Jupyter</li> <li>4. Specialized Markdown Editor</li> <li>5. Sublime Text + Markdown Preview + Chrome</li> </ul> <p></p>"},{"location":"FAQs/markdown2pdf/#1-online-markdown-to-pdf-converter","title":"1. Online Markdown to PDF Converter","text":"<p>The easiest is perhaps the cloud-based markdown to PDF converters</p> <ul> <li>https://cloudconvert.com/md-to-pdf: multipurpose converter including md to pdf.</li> <li>Dillinger.io: editing and expert to pdf, html</li> <li>Markdowntopdf.com: quick and easy</li> </ul> <p></p>"},{"location":"FAQs/markdown2pdf/#2-r-studio-requires-pandoc","title":"2. R-Studio - Requires Pandoc","text":"<p>You can open the <code>md</code> file in R-studio, and save as PDF (this is done through Pandoc).</p> <p>To install Pandoc, you can download it from Pandoc. Note that Latex software, such as MikTex is also required.</p> <p></p>"},{"location":"FAQs/markdown2pdf/#3-jupyter","title":"3. Jupyter","text":"<p>You can open the <code>md</code> file in jupyter notebook, and save as PDF (this is done through Pandoc).</p> <p>Alternatively, you may choose to print the file, then using the browser's built-in save as PDF capability. Specifically, go to File &gt; Print Preview &gt; Using the Browser's Save As PDF (e.g. Chrome) or Print + Microsoft Print to PDF (Internet Explorer). </p> <p></p>"},{"location":"FAQs/markdown2pdf/#4-specialized-markdown-editor","title":"4. Specialized Markdown Editor","text":"<p>Specialized Markdown Software such as Typora or Markdown Monster tend to have their own markdown to PDF converter.</p> <p></p>"},{"location":"FAQs/markdown2pdf/#5-sublime-text-markdown-preview-chrome","title":"5. Sublime Text + Markdown Preview + Chrome","text":"<p>If use sublime text as your text editor. There are markdown editing and preview packages you can install.</p> <ul> <li>Use sublime package control to install<ul> <li>MarkdownEditing</li> <li>Markdown Preview</li> </ul> </li> <li>Alt+M to view markdown in Chrome, Ctrl+p to print to PDF.</li> </ul>"},{"location":"FAQs/mounts3/","title":"Mount s3 folder to databricks","text":"<p>Databricks community edition has 10G limitation on how much data you can upload to databricks DBFS. If you have a bigger files you can upload to Amazon's S3 and mount it on Databricks.</p> <p>1. Create an Amazon S3 policy</p> <p>Go to AWS's IAM control panel &gt; Policy &gt; Create policy &gt; Go to JSON (tab),  &gt; paste the following policy and change <code>msbabigdata</code> (in two places) to your own bucket name. This policy gives the policy holder full access to your s3 folder.</p> <pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata\"\n            ]\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:DeleteObject\",\n                \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::msbabigdata/*\"\n            ]\n        }\n    ]\n}\n</code></pre> <p>Then give the policy a name \"s3databricks\", and Create policy.</p> <p>2. Create an IAM user and attach the policy</p> <p>Go to IAM &gt; Users &gt; Add user </p> <p>Give a user name (e.g. <code>s3databricks</code>), check \"Programmatic access\", \"Next: Permissions\"</p> <p>Click \"Attach existing policies directly\", search for \"s3databricks\" that you created, check this policy, then click \"Next:tags\"</p> <p>Accept the defaults in the next few screens, until you \"Create user\". </p> <p>Download the credential \".csv\" and save it in a safe place (you can only download it once). You will need the Access key ID, and Secret later. You can show the Secret and copy it for later use. </p> <p>3. Mount the s3 bucket in Databricks</p> <p>In a Databricks notebook, paste the following and replace the <code>ACCESS_KEY</code> and <code>SECRET_KEY</code> with the Access key ID and Secret you obtained in the above. </p> <p>Also replace the <code>AWS_BUCKET_NAME</code> with your bucket name. You may choose your own mount folder name.</p> <pre><code>ACCESS_KEY = \"xxx\"\nSECRET_KEY = \"xxxxx\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = \"msbabigdata\"\nMOUNT_NAME = \"msbabigdata\"\ntry:\n  dbutils.fs.unmount( \"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  pass\ndbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n</code></pre> <p>Remark: in general you do not want to directly provide your secret in your code , but the safer approach uses a Databricks API that is disabled in the community edition.</p> <p>Note that we did an unmount first in case you want to remount it.</p> <p>Then you may test your mounted s3 folder using DBFS utility:</p> <pre><code>%fs ls /mnt/msbabigdata\n</code></pre> <p>Later you may reference folders/files in this folder using</p> <p>\"/mnt/msbabigdata/...\"</p> <p>e.g. </p> <p><code>myRDD = sc.textFile(\"/mnt/msbabigdata/adirdata/ratings_2013.txt\")</code></p>"},{"location":"FAQs/sparkfaq/","title":"Common Issues with PySpark","text":"<ul> <li>Common Issues with PySpark<ul> <li>What if the data file I try to upload through Jupyter Notebook exceeds 25 MB?</li> </ul> </li> <li>if you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code></li> <li>Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</li> </ul>"},{"location":"FAQs/sparkfaq/#what-if-the-data-file-i-try-to-upload-through-jupyter-notebook-exceeds-25-mb","title":"What if the data file I try to upload through Jupyter Notebook exceeds 25 MB?","text":"<p>Jupyter Notebook allows you to upload files no larger than 25MB. If the file exceeds the 25 MB, you can use the wget approach (if the data file has a downloadable link). </p> <p>To use <code>wget</code> from jupyter notebook, use <code>New &gt; Terminal</code>, then you have access to the bash terminal, where you can run:</p> <pre><code>wget url_to_data_file\n</code></pre> <p>If the file is already your disk (but no in cloudera VM), you can upload to cloudera VM using the shared vagrant folder (e.g. your MSBA desktop's c:/vagrant is shared with Cloudera VM's /vagrant). </p> <ol> <li>copy your file to <code>c:/vagrant</code> on your MSBA desktop</li> <li>open bash from Cloudera VM. Type <pre><code>ls /vagrant/\n</code></pre> to verify that your file exists in the vagrant folder on your VM. </li> <li>Copy or move your file to the intended directory, e.g.:  <pre><code>cp /vagrant/file  ~/file\n</code></pre> This will copy your file to your home directory. </li> </ol> <p></p>"},{"location":"FAQs/sparkfaq/#if-you-encounter-an-error-the-root-scratch-dir-tmphive-on-hdfs-should-be-writable","title":"if you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code>","text":"<p>If you encounter an error, <code>The root scratch dir: /tmp/hive on HDFS should be writable</code>. Open a terminal and run the following comamnd: </p> <pre><code>sudo chmod 777 /tmp/hive\n</code></pre> <p></p>"},{"location":"FAQs/sparkfaq/#unable-to-instantiate-orgapachehadoophiveqlmetadatasessionhivemetastoreclient","title":"Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient","text":"<p>If at any point you see this error, <code>AnalysisException: u'java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;</code>. </p> <p>Please - removing the *.lck file from hive <code>metastore_db</code> <pre><code># assuming you're running this from your home directory from cloudera vm\nrm  metastore_db/*.lck\n</code></pre> - Terminate all other running jupyter notebooks (from your jupyter home, go to Running tab, then terminate). </p> <p>If the above does not work still, try to restart the kernel (from your current jupyter notebook's menu, kernel &gt; restart). </p>"},{"location":"FAQs/textEditor/","title":"Recommended Text Editors for Scripting &amp; Text File Handling","text":"<p>Much of the data science is text based, including plain text files, csv, JSON files, Markdown files, even jupyter notebook files, python/R scripts, and shell scripts. Having a powerful text editor can increase your productivity and empower you.</p> <p>Here are a few text editors that I have used and recommend. All of them are powerful editors with tons of features and abilities that can edit all kinds of text files as mentioned above. They all support syntax highlighting, multi-cursor editing, useful shortcuts, powerful search/replace features, etc.  All of them are free also. </p> <ul> <li>Visual Studio Code: Visual Studio Code (VS Code) is a newer and immensely popular text editor. It is cross-platform, modern, and inherits the best features of outstanding text/script editors such as sublime, and make them even more professionally done, extensible, and powerful. I switched from Sublime to VS Code recently for VS Code's built-in support for jupyter notebook. It has nearly all of the sublime features that I regularly use and more. Love it so far. </li> </ul> <ul> <li>Notepad++: beloved text/script editor for Windows users. It had been my primary editor for a long time before I switched to Sublime Text about 3 years ago. </li> </ul> <ul> <li>Sublime Text: text/script editor for all platforms (the evaluation version does not expire but it will ask you to purchase once in a while). It is beautiful, supports powerful multi-cursor editing, select all text of the same pattern, searching in all files, tons of plugins (and easy-to-install), easy to use command pallete, go to anything, and git integration. I have happily used for a while for all of my coding, git repositories, note taking, to-do list management, and the entire course production workflow. </li> </ul>"},{"location":"FAQs/viewdelimiters/","title":"View Delimiters and Invisible Characters","text":"<p>Often time you need to know whether a while space is a tab or space, and whether an invisible character has been used as delimiters.</p>"},{"location":"FAQs/viewdelimiters/#linux","title":"Linux","text":"<p>You can use <code>cat -t</code> including (<code>cat -t | head</code>) to show invisible characters</p> <p>e.g.:</p> <p><code>cat -t sample_artist_data.txt | head</code></p> <p><pre><code>1134999^I^A06Crazy^B Life\n6821360^IPang^C Nakarin\n10113088^ITerfel, Bartoli- Mozart: Don\n10151459^IThe Flaming Sidebur\n6826647^IBodenstandig 3000\n</code></pre> where  - <code>^I</code>: tab (<code>\\t</code>) - <code>^A</code>: control-A (<code>\\001</code>) - <code>^B</code>: control-B (<code>\\002</code>) - <code>^C</code>: control-C (<code>\\003</code>)</p>"},{"location":"FAQs/viewdelimiters/#windows","title":"Windows","text":"<p>In windows, with Notepad++, turn on the \"Show All Characters\" (menu&gt; view &gt; show symbol &gt; show all characters) </p> <p></p> <p>In sublime, if you select text, it will show invisible characters including space and tab.</p> <p></p> <p>Additionally, you may use regex search (<code>\\t</code>, <code>\\n</code>, <code>\\r</code>,<code>\\001</code> etc) to highlight special characters.</p> <p></p>"},{"location":"FAQs/windows_cmd/","title":"Windows Commands 101","text":"<p>This short tutorial introduces the basics of command line operations in Windows OS. </p>"},{"location":"FAQs/windows_cmd/#1-open-a-command-window-window","title":"1. Open a command window window","text":"<p>To open a command window at a given directory, </p> <ul> <li>Windows: go to the directory in the file explorer, Shift+Right click, you\u2019ll see \"Open a command window here\". <ul> <li>If you have git installed. You may also right-click and \"open a git bash window\", which is another flavor of command window.</li> </ul> </li> <li>Mac OS: you can enable the \"open terminal here\" service and access the service using Right Click (or Control + Click), then services (more detail here http://goo.gl/cmMmz).</li> </ul>"},{"location":"FAQs/windows_cmd/#2-commonly-used-windows-commands","title":"2. Commonly used Windows commands","text":"Purpose windows Mac OS view content of current directory <code>dir</code> <code>ls</code> <code>ls -l</code> go to a subdirectory* <code>cd dir_name</code> <code>cd dir_name</code> go one level up <code>cd ..</code> <code>cd ..</code> what is the current directory <code>dir</code> <code>pwd</code> <p>Note: in many cases, you can type tab to autocomplete the dir/file name</p>"},{"location":"FAQs/windows_cmd/#3-run-python-script-from-command-window","title":"3. Run python script from command window","text":"<pre><code>python python_file_name.py\n</code></pre>"}]}